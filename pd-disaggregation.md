# PD Disaggregation in LLM Inference

This document introduces the concept of Prefill‚ÄìDecode (PD) disaggregation in large language model (LLM) inference, its benefits, current implementation status in common AI Infra projects, and the future roadmap for its adoption.

## Table of Contents

- [What is PD Disaggregation](#what-is-pd-disaggregation)
- [Project Support Status](#project-support-status)
  - [NVIDIA Dynamo](#nvidia-dynamo)
  - [vLLM](#vllm)
  - [AIBrix](#aibrix)
  - [InftyAI llmaz](#inftyai-llmaz)
- [Roadmap](#roadmap)

---

## What is PD Disaggregation

In LLM inference, the process can be divided into two distinct phases:

- **Prefill**: processes the entire input prompt in parallel, builds KV cache.
- **Decode**: generates output tokens one by one using the KV cache.

In a monolithic setup, both prefill and decode run on the same GPU, which causes resource contention (e.g., prefill latency impacting decode throughput). **PD Disaggregation** addresses this by disaggregating prefill and decode phases into separate GPU workers or nodes.

**Benefits:**

- Improved latency (TTFT and TPOT)
- Higher throughput per GPU
- Independent scaling and tuning of each phase
- Flexibility for scheduling and load balancing

---

## Project Support Status

### NVIDIA Dynamo

- ‚úÖ **Full support** for PD disaggregation (built-in)
- Uses **NIXL** for efficient KV cache transfer between GPUs
- Supports *conditional disaggregation* (hybrid mode)
- Production-ready with Kubernetes operator
- Actively maintained and part of NVIDIA‚Äôs AI Enterprise roadmap

### vLLM (with LMCache)

- ‚ùå Core vLLM does **not** support PD disaggregation natively
- ‚úÖ **Supported** via [LMCache](https://github.com/LMCache/lmcache) plugin stack (uses NIXL)
- Experimental stage; production usage seen in LMCache production stack
- vLLM community is exploring deeper native integration

### AIBrix

- üü° **Indirect support**
- Distributed KV pool enables PD scenarios
- Roadmap includes dedicated prefill/decode workers
- Currently focuses on control-plane and orchestration over vLLM

### InftyAI/llmaz

- ‚úÖ **Explicit support**
- `xPyD` mode (exclusive Prefill‚ÄìDecode): supports multi-host disaggregated serving
- Homogeneous PD disaggregation available from alpha version
- Production intent, but still alpha with evolving APIs


## References

- https://github.com/NVIDIA/dynamo
- https://github.com/vllm-project/vllm
- https://github.com/vllm-project/production-stack
- https://github.com/vllm-project/aibrix
- https://github.com/InftyAI/llmaz
- https://github.com/LMCache/lmcache
- DistServe (OSDI‚Äô24): https://www.usenix.org/system/files/osdi24-zhong-yinmin.pdf

**Some were generated by ChatGPT. So please be careful before you use them. This is a personal learning notes.**